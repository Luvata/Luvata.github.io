<h1 id="hidden-markov-model">Hidden Markov Model</h1>
<h2 id="tldr">TL;DR</h2>
<p>Hidden Markov Model (HMM) is - A probabilistic sequence model, augmenting <a href="Markov%20Chain.md">Markov Chain</a>, where the <strong>actual state is unknown</strong> - Has three characteristics: - Likelihood (Similar with <a href="Language%20Model.md">Language Model</a>) - Decoding (Discover best hidden state) - Learning (or training) - Applications: Sequence Labeling problems: <a href="Named%20Entity%20Recognition.md">Named Entity Recognition</a>, <a href="Automatic%20Speech%20Recognition.md">Automatic Speech Recognition</a>, …</p>
<h2 id="definition">Definition</h2>
<ul>
<li>Beside states <span class="math inline"><em>Q</em></span>, state-transition matrix <span class="math inline"><em>A</em></span> and initial probability distribution <span class="math inline"><em>π</em></span>, HMM extends Markov chain with:
<ul>
<li><span class="math inline"><em>O</em></span> <strong>Observations</strong> - Only information of current state that we know</li>
<li><span class="math inline"><em>B</em> = <em>b</em><sub><em>i</em></sub>(<em>o</em><sub><em>t</em></sub>)</span> The <strong>observation likelihood</strong>, or <strong>emission probability</strong>, <span class="math inline"><em>P</em>(<em>o</em><sub><em>t</em></sub>|<em>b</em><sub><em>i</em></sub>)</span> probability of an observation <span class="math inline"><em>o</em><sub><em>t</em></sub></span> that was generated by state <span class="math inline"><em>q</em><sub><em>i</em></sub></span></li>
</ul></li>
<li>In other words, HMM parameters are <span class="math inline"><em>A</em></span> - state transition matrix and <span class="math inline"><em>B</em></span> - observation likelihood</li>
<li>Assumptions for first-order HMM:
<ul>
<li>Markov assumption</li>
<li>Output independence: Probability of an ouput observation <span class="math inline"><em>o</em><sub><em>i</em></sub></span> <strong>only depends</strong> on the state <span class="math inline"><em>q</em><sub><em>i</em></sub></span></li>
</ul></li>
</ul>
<h3 id="likelihood-estimation">Likelihood Estimation</h3>
<p>Given a HMM <span class="math inline"><em>λ</em></span>, and a sequence of observation <span class="math inline"><em>O</em></span>, find likelihood of that sequence</p>
<p>For <strong>N</strong> hidden states and <strong>T</strong> observations, we can try to calculate all <span class="math inline"><em>N</em><sup><em>T</em></sup></span> possible sequences of state that can induce <span class="math inline"><em>O</em></span>, then sum all the probability to get the likelihood of that sequence - very inefficient</p>
<p>Another approach (which is more doable) is using <strong>forward algorithm</strong> with complexity <span class="math inline"><em>O</em>(<em>N</em><sup>2</sup><em>T</em>)</span>.</p>
<figure>
<img src="figure/HMM_likelihood_draw.png" alt="" /><figcaption>HMM Likelihood</figcaption>
</figure>
<h2 id="references">References</h2>
<ol type="1">
<li>Hidden Markov Model <a href="https://web.stanford.edu/~jurafsky/slp3/A.pdf">slp3 - Appendix A</a></li>
</ol>
